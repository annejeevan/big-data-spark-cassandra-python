{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Importing libraries which are needed\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must now load the data (hvac.csv), parse it, and use it to train the model. \n",
    "For this,you define a function that checks whether the actual temperature of the building is greater than the \n",
    "target temperature. If the actual temperature is greater, the building is hot, denoted by the value 1.0. \n",
    "If the actual temperature is lesser, the building is cold, denoted by the value 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List the structure of data for better understanding. Because the data will be\n",
    " # loaded as an array, this structure makes it easy to understand what each element\n",
    " # in the array corresponds to\n",
    "\n",
    " # 0 Date\n",
    " # 1 Time\n",
    " # 2 TargetTemp\n",
    " # 3 ActualTemp\n",
    " # 4 System\n",
    " # 5 SystemAge\n",
    " # 6 BuildingID\n",
    "\n",
    "LabeledDocument = Row(\"BuildingID\", \"SystemInfo\", \"label\")\n",
    "\n",
    " # Define a function that parses the raw CSV file and returns an object of type LabeledDocument\n",
    "\n",
    "def parseDocument(line):\n",
    "    values = [str(x) for x in line.split(',')]\n",
    "    if (values[3] > values[2]):\n",
    "         hot = 1.0\n",
    "    else:\n",
    "         hot = 0.0        \n",
    "\n",
    "    textValue = str(values[4]) + \" \" + str(values[5])\n",
    "\n",
    "    return LabeledDocument((values[6]), textValue, hot)\n",
    "\n",
    " # Load the raw HVAC.csv file, parse it using the function\n",
    "data = sc.textFile(\"/home/osboxes/data/SensorFiles/HVAC.csv\")\n",
    "\n",
    "documents = data.filter(lambda s: \"Date\" not in s).map(parseDocument)\n",
    "training = documents.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the Spark machine learning pipeline that consists of three stages: tokenizer, hashingTF, and lr. \n",
    "For more information about what is a pipeline and how it works see - http://spark.apache.org/docs/latest/ml-pipeline.html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"SystemInfo\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline to the training document. Verify the training document to checkpoint your progress with the application. press SHIFT + ENTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|BuildingID|SystemInfo|label|\n",
      "+----------+----------+-----+\n",
      "|         4|     13 20|  0.0|\n",
      "|        17|      3 20|  0.0|\n",
      "|        18|     17 20|  1.0|\n",
      "|        15|      2 23|  0.0|\n",
      "|         3|      16 9|  1.0|\n",
      "|         4|     13 28|  0.0|\n",
      "|         2|     12 24|  0.0|\n",
      "|        16|     20 26|  1.0|\n",
      "|         9|      16 9|  1.0|\n",
      "|        12|       6 5|  0.0|\n",
      "|        15|     10 17|  1.0|\n",
      "|         7|      2 11|  0.0|\n",
      "|        15|      14 2|  1.0|\n",
      "|         6|       3 2|  0.0|\n",
      "|        20|     19 22|  0.0|\n",
      "|         8|     19 11|  0.0|\n",
      "|         6|      15 7|  0.0|\n",
      "|        13|      12 5|  0.0|\n",
      "|         4|      8 22|  0.0|\n",
      "|         7|      17 5|  0.0|\n",
      "+----------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(training)\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the actual temperature is less than the target temperature suggesting the building is cold. Hence in the training output, the value for label in the first row is 0.0, which means the building is not hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a data set to run the trained model against. To do so, we would pass on a system ID and system age (denoted as SystemInfo in the training output), and the model would predict whether the building with that system ID and system age would be hotter (denoted by 1.0) or cooler (denoted by 0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SystemInfo here is a combination of system ID followed by system age\n",
    "Document = Row(\"id\", \"SystemInfo\")\n",
    "test = sc.parallelize([(1L, \"20 25\"),\n",
    "              (2L, \"4 15\"),\n",
    "              (3L, \"16 9\"),\n",
    "              (4L, \"9 22\"),\n",
    "              (5L, \"17 10\"),\n",
    "              (6L, \"7 22\")]) \\\n",
    "    .map(lambda x: Document(*x)).toDF() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(SystemInfo=u'20 25', prediction=0.0, probability=DenseVector([0.5001, 0.4999]))\n",
      "Row(SystemInfo=u'4 15', prediction=0.0, probability=DenseVector([0.5018, 0.4982]))\n",
      "Row(SystemInfo=u'16 9', prediction=1.0, probability=DenseVector([0.4787, 0.5213]))\n",
      "Row(SystemInfo=u'9 22', prediction=1.0, probability=DenseVector([0.455, 0.545]))\n",
      "Row(SystemInfo=u'17 10', prediction=1.0, probability=DenseVector([0.4927, 0.5073]))\n",
      "Row(SystemInfo=u'7 22', prediction=0.0, probability=DenseVector([0.5017, 0.4983]))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents and print columns of interest\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"SystemInfo\", \"prediction\", \"probability\")\n",
    "for row in selected.collect():\n",
    "    print row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
